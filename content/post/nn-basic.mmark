---
date: 2017-04-19
subtitle: "Basic Concepts"
tags: ["theory"]
title: "Neural Network"
---

This is a fully detailed note about basic neural network concepts, including loss functions, activation functions, back propagation and so on.
<!--more-->

### The Neural Network Model

Online demos to interatively training your own neural network on a toy dataset with cool layer-by-layer visualization can be found in: 

- [ConvertJS](http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html).
- [tensorflow playground](http://playground.tensorflow.org/).

Which can give you a first impression on what neural networks do.

Most common design involves the stacking of affine transformations followed by element-wise non-linearity.

<div align="center">
    <img src="/img/nn/nn-intro.png" alt="nn-intro" width="500">
</div>

Forward propagation:

$$\begin{align}
z^{(l+1)} &= W^{(l)} a^{(l)} + b^{(l)}\\
a^{(l+1)} &= f(z^{(l+1)})\\
h_{W,b}(x) &= a^{(n_l)}
\end{align}$$

The input layer can be embeddings of raw entities, which can be treated as resulting from an "embedding layer" or "lookup layer". The weights of the output layer can be seen as embeddings of output classes.

### Loss functions
#### Square Loss
$$
J(\theta)=
 \frac{1}{m} \sum_{i=1}^m \left( \frac{1}{2} \left\| h_{\theta}(x^{(i)}) - y^{(i)} \right\|^2 \right)
$$

It's equivalant to maximizing the log-likelihood of a conditional Gaussian distribution.

#### Cross-Entropy Loss

$$J(\theta)=
 \frac{1}{m} \sum_{i=1}^m 
 -\log\left(\hat y_t^{(i)}\right)$$

where $$t$$ is the index of the correct class. 
When using cross-entropy loss, it is assumed that the network’s output is transformed using the softmax transformation, in which case increasing the mass assigned to the correct class means decreasing the mass assigned to all the other classes.
Label smoothing regularizes a model based on a softmax with $$k$$ output values by replacing the hard 0 and 1 classification targets with targets of $$\frac{\epsilon}{k}$$ and $$1−\frac{k−1}{k}\epsilon$$, respectively. It prevents the model to learn larger and larger weights, making more extreme predictions forever to reach the unreachable 0 and 1 target without discouraging correct classification (unlike weight decay).
#### Hinge Loss
$$
J(\theta)=
 \frac{1}{m} \sum_{i=1}^m 
 \max\left(0,1-\left(\hat y_t^{(i)} - \hat y_k^{(i)}\right) \right)
$$

where $$k=\operatorname{argmax}_{j\neq t}\hat y_j$$. Hinge loss attempts to score the correct class above all other classes with a margin of at least 1.
#### Ranking Loss (hinge)
The goal is to score correct items above incorrect ones, given pairs of correct and incorrect items $$x_p$$ and $$x_n$$. Such training situations arise when we have only positive examples, and generate negative examples by corrupting a positive example.

$$
J(\theta)=
\frac{1}{m} \sum_{i=1}^m\max \left( 0,1- \left( h_{\theta}(x^{(i)}_p)-h_{\theta}(x^{(i)}_n)\right) \right)
$$

The objective is to score correct inputs over incorrect ones with a margin of at least 1.
#### Ranking Loss (log)

<div>
$$
J(\theta)=
\frac{1}{m} \sum_{i=1}^m\log \left( 1+ \exp\left(-\left( h_{\theta}(x^{(i)}_p)-h_{\theta}(x^{(i)}_n)\right)\right) \right)
$$
</div>

#### Rating Prediction
Assume that all the ratings lie in $$[1,K]$$. Real-valued scores are allowed for ground-truth ratings that are an average over the evaluations of several human annotators. 
Using a softmax layer as the last layer, we get a probability output $$\hat p$$, which is seen as the probability of each discrete integer rating. We predict the scoring by $$\hat y = r^T\hat p$$, where $$r^T=[1\ 2\ \dots\ K]$$, and optimize the model over the following cost function:

$$
J(\theta)=\frac{1}{m}\sum_{k=1}^m 
\operatorname{KL}\left(
  p^{(k)}\left\|\hat p^{(k)}
\right)\right.
+\frac{\lambda}{2}\|\theta\|_2^2\ .
$$

The sparse target distribution $$p$$ is defined as:

$$
\begin{align}
p_i = 
\begin{cases} 
1-\left(y-\lfloor y\rfloor\right), \ &i=\lfloor y\rfloor \\
y-\lfloor y\rfloor,    &i=\lfloor y\rfloor+1\\
0    &\mbox{otherwise }
\end{cases}
\end{align}
$$

for $$1\leq i\leq K$$. For example, if $$K=5$$ and $$y=3.6$$, $$p=[0, 0, 0.4, 0.6, 0]$$. See [this paper](http://arxiv.org/pdf/1503.00075.pdf).

### Activation function 
$$f(\sum_iW_ix_i+b)\equiv f(z)$$
![activations](/img/nn/activations.png)
#### Sigmoid function
$$
f(z) = \frac{1}{1+\exp(-z)}
$$

whose derivative is $$f'(z) = f(z) (1-f(z))$$ ($$=a(1-a)$$ in implementation). The derivative of the softmax funtion is the same. 
An interesting property is $$1-f(z) = f(-z)$$ (which can be seen intuitively from the plot above).
#### Hyperbolic tangent (rescaled sigmoid)

$$\begin{align}
f(z) &= \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\\
&= 2\operatorname{Sigmoid}(2z)-1.
\end{align}$$

whose derivative is $$f'(z) = 1- (f(z))^2$$ ($$=1-a^2$$ in implementation).
Hyperbolic tangent is better than sigmoid function in most cases. It resembles the identity function more closely which makes training the tanh network easier.

The widespread saturation of sigmoidal units can make gradient-based learning very difficult. For this reason, their use as hidden units in feedforward networks is now discouraged. However, in models holding additional requirements that rule out the use of piecewise linear activation functions, sigmoidal units are still appealing despite the drawbacks of saturation.

#### Hard tanh
$$
\begin{align}
f(z)= 
\begin{cases} 
-1 \ &\mbox{if } x<-1 \\
0    &\mbox{if } -1<=x<=1\\
1    &\mbox{if } x>1
\end{cases}
\end{align}
$$

Hard tangent performs silimarly to tanh, but is computationally cheaper.

#### Rectified linear function (ReLU)
(neither bounded nor continuously differentiable)

$$
f(z) = \max(0,x).
$$

Rectified linear units and all of these generalizations of them are based on the principle that models are easier to optimize if their behavior is closer to linear.
Combine with ReLu, it's better to initialize $$W$$ with identity matrix or its scaled version and initialize biases with zeros (starting from simple averaging). 
As a rule of thumb, ReLU units work better than tanh, and tanh works better than sigmoid.
#### Generalizations of ReLU
**Slope methods**

$$
f(z)=\max(0,z)+\alpha\min(0,z)
$$

* Absolute value rectification: \\(\alpha=-1\\), \\(f(z)=|z|\\). It is used in object recognition from images, where it makes sense to seek features that are invariant under a polarity reversal of the input illumination
* Leaky ReLU: fix \\(\alpha\\) to a small value like 0.01.
* Parametric ReLU (PReLu): treats \\(\alpha\\) as a learnable parameter.

**Exponential linear units (ELU)** 

$$
\begin{align}f(z)= 
\begin{cases} 
x \ &\mbox{if } x\geq 0 \\
\alpha (e^x-1)    &\mbox{if }x<0
\end{cases}\end{align}
$$

**Maxout networks**

$$
f(z) = \max_{j\in[1,k]}W_j^Tx+b_j.
$$

If $$k=2$$, it simplifies to 

$$
f(z) = \max(W_1^Tx+b_1, W_2^Tx+b_2)
$$

![maxout](/img/nn/maxout.png)
All slope methods are special cases of this form. The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU). However, unlike the ReLU neurons it multiplies the number of parameters for every single neuron, leading to a high total number of parameters.
#### Indentical function
It's equivalant to compute:

$$
h = f(VUx+b)
$$

If $$U$$ produces $$q$$ outputs, then $$U$$ and $$V$$ together contain only $$(n + p)q$$ parameters, while $$W$$ contains $$np$$ parameters. For small $$q$$, this can be a considerable saving in parameters. It comes at the cost of constraining the linear transformation to be **low-rank**, but these low-rank relationships are often sufficient. Linear hidden units thus offer an effective way of reducing the number of parameters in a network.
#### Last words for activation functions
It is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.

"What neuron type should I use?" Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of "dead" units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.


### Optimization
The basic update rule for gradient descent:

$$
\begin{align}
W_{ij}^{(l)} &= W_{ij}^{(l)} - \alpha \frac{\partial}{\partial W_{ij}^{(l)}} J(W,b) \\
b_{i}^{(l)} &= b_{i}^{(l)} - \alpha \frac{\partial}{\partial b_{i}^{(l)}} J(W,b)
\end{align}
$$

where $$\alpha$$ is the learning rate. The **back propagation** algorithm gives an efficient way to compute these partial derivatives. The idea behind back propagation is that we can re-use derivatives computed for higher layers in computing derivatives for lower layers (in terms of back propagated error).
The computation of the partial derivative for a single example is as follows (and computation is usually parallelized):

1. Perform a feedforward pass, computing the activations for layers $$L_2$$, $$L_3$$, up to the output layer $$L_{nl}$$, using the equations defining the forward propagation steps
2. For the output layer (layer $$n_l$$), set $$ \delta^{(n_l)} =a^{(n_l)}-y$$
3. For $$l=n_l−1,n_l−2,n_l−3,\dots,2$$ (there's no "error" in input layer), set
   $$\delta^{(l)} = \left((W^{(l)})^T \delta^{(l+1)}\right) \bullet f'(z^{(l)})$$
4. Compute the desired partial derivatives for a single training example:

$$
\begin{align}
\nabla_{W^{(l)}} J(W,b;x,y) &= \delta^{(l+1)} (a^{(l)})^T+\lambda W^{(l)}, \\
\nabla_{b^{(l)}} J(W,b;x,y) &= \delta^{(l+1)}.
\end{align}
$$

The $$\bullet$$ denotes the element-wise product operator (also called the Hadamard product). The same extension goes for $$f(\cdot)$$ and $$f'(\cdot)$$. $$\lambda$$ is the regularization parameter. The partial derivatives are then sum up (and normalized by the number of training examples if they should match the cost function) to get the partial derivative w.r.t a whole batch (or minibatch). Since the back propagation is hard to implement and prone to tiny errors and bugs, remember to perform [gradient checking](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization) after implementation. View [this](/img/nn/gradient-checking.png) common asked question about gradient checking. 

A clean python implementation and illustration for neural network can be found [here](/html/nn/demo.html).

To learn about more neural network architectures, see [this blog post](https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba).

To learn about advanced topics on neural networks, click "Next Post" below :)

### Reference
- [Stanford CS224d Lecture 5](http://cs224d.stanford.edu/lectures/CS224d-Lecture5.pdf)
- [Stanford CS224d Lecture 6](http://cs224d.stanford.edu/lectures/CS224d-Lecture6.pdf)
- [Stanford CS224d Lecture 8](http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf) ([Vanishing gradient example](http://cs224d.stanford.edu/notebooks/vanishing_grad_example.html))
- [Stanford CS224d Midterm Solutions](http://cs224d.stanford.edu/midterm/midterm_solutions.pdf)
- Stanford CS231n: [Minimal net example](http://cs231n.github.io/neural-networks-case-study/)
- Stanford CS231n: [Neural Networks Part 1: Setting up the Architecture](http://cs231n.github.io/neural-networks-1/#intro)
- http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/
- [A primer on neural network models for natural language processing](http://arxiv.org/pdf/1510.00726)
- [Deep Learning Book](http://www.deeplearningbook.org/)
